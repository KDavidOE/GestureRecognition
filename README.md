# GestureRecognition
This project provides a software to control PowerPoint presentations using hand gestures.

The main goal is to create a robust system that can tackle difficulties such as dynamic background.
Dynamic background could be the occurance of multiple moving hands in a frame, different skin colors, not sufficient lighting and shadows.

The solution provides the followings
- A Convolutional Neural Network
    - Based on 4 block layers
    - 3 blocks constist of max-pooling and convolutional layers
    - Final block is made out of a flatten and a dense layer
- A Conditional Generative Adverserial Network.
    - The generator is a U-net decoder
    - The discriminator is a "Markovian patch-discriminator"
- Various image processing solutions such as
    - Optical flow
    - Google's MediaPipe
    - Gauss filter, color thresholding etc.
- The GUI is implemented using PySimpleGui, and the system calls performed with PyWin32
- The input data dimensions: 64 x 64 x 3

Class diagram:

![image](https://github.com/KDavidOE/GestureRecognition/assets/101677036/ef45f8f9-b502-48e4-8906-91d411fa0574)

# Trained CNN and C-GAN results using American Sign Language with custom dataset.

| ![image](https://github.com/KDavidOE/GestureRecognition/assets/101677036/0c0b4246-7de5-4250-9e61-9fde25835264) |
|:--:|
| Applied masking methods on dataset to train C-GAN |

| ![image](https://github.com/KDavidOE/GestureRecognition/assets/101677036/26fe0bcb-7362-486f-a437-4d8674173589)| 
|:--:| 
| First row: input images \| Second row: Images regenerated by C-GAN |


| ![image](https://github.com/KDavidOE/GestureRecognition/assets/101677036/58e83597-4821-47f8-92d1-fe59981c4db1) | ![image](https://github.com/KDavidOE/GestureRecognition/assets/101677036/549a8a89-9736-4bba-9fe9-1a6e29f8b0c3) |
| :---:| --- |


# References
- B. Jason, „Implementation of pix2pix with keras.” https:// machinelearningmastery.com/how-to-implement-pix2pix- gan-models-from-scratch-with-keras/
- D. Dahmani, M. Cheref, and S. Larabi, „Zero-sum game theory model for segmenting skin regions,” Image and Vision Computing, vol. 99, p. 103925, 2020
- O. Ronneberger, P. Fischer, and T. Brox, „U-net: Convolutional networks for bio- medical image segmentation,” in Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pp. 234–241, Springer, 2015
- Google, „On-device, real-time hand tracking with mediapipe.” https: //ai.googleblog.com/2019/08/on-device-real-time-hand- tracking-with.html
# Note
- If the network models or the datasets are needed, contact me.
